{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ğŸ‡¹ğŸ‡· Nutuk RAG Sistemi (Google Colab Versiyonu)\n",
    "\n",
    "Bu not defteri, Nutuk Ã¼zerinde soru-cevap yapabilen bir RAG (Retrieval-Augmented Generation) sistemini Google Colab Ã¼zerinde Ã§alÄ±ÅŸtÄ±rmak iÃ§in hazÄ±rlanmÄ±ÅŸtÄ±r.\n",
    "\n",
    "### âš ï¸ Ã–nemli Not\n",
    "Bu sistemin hÄ±zlÄ± Ã§alÄ±ÅŸabilmesi iÃ§in **GPU** gereklidir.\n",
    "LÃ¼tfen yukarÄ±daki menÃ¼den **Runtime > Change runtime type** (Ã‡alÄ±ÅŸma zamanÄ± > Ã‡alÄ±ÅŸma zamanÄ± tÃ¼rÃ¼nÃ¼ deÄŸiÅŸtir) seÃ§eneÄŸine gidin ve **Hardware accelerator** (DonanÄ±m hÄ±zlandÄ±rÄ±cÄ±) olarak **T4 GPU**'yu seÃ§in."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gerekli kÃ¼tÃ¼phanelerin kurulumu\n",
    "print(\"ğŸ“¦ Gerekli kÃ¼tÃ¼phaneler kuruluyor... (Bu iÅŸlem birkaÃ§ dakika sÃ¼rebilir)\")\n",
    "!pip install -q langchain langchain-community pypdf chromadb sentence-transformers huggingface_hub\n",
    "\n",
    "# GPU desteÄŸi ile llama-cpp-python kurulumu\n",
    "!CMAKE_ARGS=\"-DGGML_CUDA=on\" pip install -q llama-cpp-python\n",
    "\n",
    "print(\"âœ… Kurulum tamamlandÄ±.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import logging\n",
    "from google.colab import files\n",
    "\n",
    "# Loglama yapÄ±landÄ±rmasÄ±\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# Gerekli paketleri kontrol et\n",
    "try:\n",
    "    from huggingface_hub import hf_hub_download\n",
    "    from langchain_community.document_loaders import PyPDFLoader\n",
    "    from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "    from langchain_community.vectorstores import Chroma\n",
    "    from langchain_community.embeddings import HuggingFaceEmbeddings\n",
    "    from langchain_community.llms import LlamaCpp\n",
    "    from langchain.chains import RetrievalQA\n",
    "    from langchain.prompts import PromptTemplate\n",
    "except ImportError as e:\n",
    "    logger.error(f\"Gerekli paket eksik: {e}\")\n",
    "    sys.exit(1)\n",
    "\n",
    "# YapÄ±landÄ±rma\n",
    "MODEL_REPO = \"microsoft/Phi-3-mini-4k-instruct-gguf\"\n",
    "MODEL_FILENAME = \"Phi-3-mini-4k-instruct-q4.gguf\"\n",
    "MODEL_DIR = \"/content/models\"\n",
    "MODEL_PATH = os.path.join(MODEL_DIR, MODEL_FILENAME)\n",
    "VECTOR_DB_DIR = \"/content/vectordb_phi3\"\n",
    "PDF_PATH = \"/content/nutuk.pdf\"\n",
    "\n",
    "def check_and_upload_pdf():\n",
    "    \"\"\"PDF dosyasÄ±nÄ± kontrol eder, yoksa yÃ¼kleme ister.\"\"\"\n",
    "    if not os.path.exists(PDF_PATH):\n",
    "        print(f\"âŒ '{PDF_PATH}' bulunamadÄ±.\")\n",
    "        print(\"ğŸ“‚ LÃ¼tfen 'nutuk.pdf' dosyasÄ±nÄ± yÃ¼kleyin:\")\n",
    "        uploaded = files.upload()\n",
    "        for filename in uploaded.keys():\n",
    "            if filename != \"nutuk.pdf\":\n",
    "                os.rename(filename, \"nutuk.pdf\")\n",
    "                print(f\"âœ… Dosya 'nutuk.pdf' olarak yeniden adlandÄ±rÄ±ldÄ±.\")\n",
    "        \n",
    "        if not os.path.exists(PDF_PATH):\n",
    "            logger.error(\"âŒ PDF dosyasÄ± yÃ¼klenmedi.\")\n",
    "            sys.exit(1)\n",
    "    else:\n",
    "        print(f\"âœ… '{PDF_PATH}' bulundu.\")\n",
    "\n",
    "def download_model():\n",
    "    \"\"\"Phi-3 Mini GGUF modeli mevcut deÄŸilse indirir.\"\"\"\n",
    "    if not os.path.exists(MODEL_PATH):\n",
    "        logger.info(f\"â¬‡ï¸ {MODEL_FILENAME} indiriliyor...\")\n",
    "        os.makedirs(MODEL_DIR, exist_ok=True)\n",
    "        hf_hub_download(repo_id=MODEL_REPO, filename=MODEL_FILENAME, local_dir=MODEL_DIR)\n",
    "        logger.info(\"âœ… Model indirildi.\")\n",
    "    else:\n",
    "        logger.info(\"âœ… Model zaten mevcut.\")\n",
    "\n",
    "def setup_rag():\n",
    "    \"\"\"PDF'i iÅŸler, embedding'leri oluÅŸturur ve RAG zincirini kurar.\"\"\"\n",
    "    \n",
    "    # 1. PDF YÃ¼kle\n",
    "    logger.info(f\"ğŸ“„ PDF yÃ¼kleniyor: {PDF_PATH}\")\n",
    "    loader = PyPDFLoader(PDF_PATH)\n",
    "    documents = loader.load()\n",
    "    logger.info(f\"   {len(documents)} sayfa yÃ¼klendi.\")\n",
    "\n",
    "    # 2. Metni ParÃ§ala\n",
    "    logger.info(\"âœ‚ï¸ Metin parÃ§alanÄ±yor...\")\n",
    "    text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
    "    texts = text_splitter.split_documents(documents)\n",
    "    logger.info(f\"   {len(texts)} parÃ§a oluÅŸturuldu.\")\n",
    "\n",
    "    # 3. Embedding'leri OluÅŸtur\n",
    "    logger.info(\"ğŸ§  Embedding modeli yÃ¼kleniyor (all-MiniLM-L6-v2)...\")\n",
    "    embeddings = HuggingFaceEmbeddings(model_name=\"all-MiniLM-L6-v2\")\n",
    "    \n",
    "    # 4. VektÃ¶r VeritabanÄ±\n",
    "    logger.info(\"ğŸ’¾ VektÃ¶r veritabanÄ± oluÅŸturuluyor (ChromaDB)...\")\n",
    "    db = Chroma.from_documents(texts, embeddings, persist_directory=VECTOR_DB_DIR)\n",
    "    \n",
    "    # 5. LLM YÃ¼kle\n",
    "    logger.info(\"ğŸ¤– Phi-3 Mini modeli yÃ¼kleniyor...\")\n",
    "    llm = LlamaCpp(\n",
    "        model_path=MODEL_PATH,\n",
    "        temperature=0.1,\n",
    "        max_tokens=512,\n",
    "        n_ctx=4096,\n",
    "        n_gpu_layers=-1, # GPU kullanÄ±mÄ±\n",
    "        verbose=False\n",
    "    )\n",
    "\n",
    "    # 6. QA Zincirini Kur\n",
    "    template = \"\"\"<|user|>\n",
    "AÅŸaÄŸÄ±daki baÄŸlamÄ± kullanarak soruyu cevapla. CevabÄ± bilmiyorsan bilmiyorum de.\n",
    "\n",
    "BaÄŸlam:\n",
    "{context}\n",
    "\n",
    "Soru:\n",
    "{question}\n",
    "<|end|>\n",
    "<|assistant|>\n",
    "\"\"\"\n",
    "    prompt = PromptTemplate(template=template, input_variables=[\"context\", \"question\"])\n",
    "\n",
    "    qa = RetrievalQA.from_chain_type(\n",
    "        llm=llm,\n",
    "        chain_type=\"stuff\",\n",
    "        retriever=db.as_retriever(search_kwargs={\"k\": 3}),\n",
    "        return_source_documents=True,\n",
    "        chain_type_kwargs={\"prompt\": prompt}\n",
    "    )\n",
    "    \n",
    "    return qa\n",
    "\n",
    "def main():\n",
    "    print(\"ğŸš€ Nutuk RAG Sistemi BaÅŸlatÄ±lÄ±yor (Google Colab Versiyonu)...\")\n",
    "    \n",
    "    check_and_upload_pdf()\n",
    "    download_model()\n",
    "    \n",
    "    qa_chain = setup_rag()\n",
    "    \n",
    "    print(\"\\nâœ… Sistem hazÄ±r! Nutuk hakkÄ±nda sorular sorabilirsiniz. (Ã‡Ä±kÄ±ÅŸ iÃ§in 'q' veya 'exit')\")\n",
    "    print(\"-\" * 50)\n",
    "    \n",
    "    while True:\n",
    "        try:\n",
    "            query = input(\"\\nSoru: \").strip()\n",
    "            if not query:\n",
    "                continue\n",
    "            if query.lower() in ['q', 'exit', 'quit']:\n",
    "                print(\"ğŸ‘‹ GÃ¶rÃ¼ÅŸmek Ã¼zere!\")\n",
    "                break\n",
    "            \n",
    "            print(\"â³ DÃ¼ÅŸÃ¼nÃ¼yor...\")\n",
    "            res = qa_chain.invoke({\"query\": query})\n",
    "            answer = res['result']\n",
    "            \n",
    "            print(f\"\\nğŸ¤– Cevap: {answer}\")\n",
    "                \n",
    "        except KeyboardInterrupt:\n",
    "            print(\"\\nğŸ‘‹ Ä°ÅŸlem iptal edildi.\")\n",
    "            break\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Bir hata oluÅŸtu: {e}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}